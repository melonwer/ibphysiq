{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10"}},"cells":[{"cell_type":"markdown","metadata":{},"source":["# Setup local model and expose a temporary Colab endpoint (ngrok)\n","\n","This notebook contains copy/paste cells you can run in Google Colab (or locally) to:\n\n- install dependencies,\n- download the safetensors file from Hugging Face,\n- convert it to a ggml/gguf binary (recommended q4_0 quantization),\n- start a local text-generation server (text-generation-webui or llama.cpp),\n- expose the server with ngrok and print the public URL you can use as `LOCAL_TGI_URL`.\n\n**Important:** conversion and hosting are resource intensive. Colab sessions are ephemeral — this setup is for development and testing only, not production. Mark long-running cells before running.\n"]},{"cell_type":"markdown","metadata":{},"source":["## 1) Prerequisites / notes\n\n- This notebook is targeted at Colab or macOS (Apple Silicon). On Colab you *do not* need to create a venv; use `pip` in the notebook runtime.\n- If running locally on macOS, prefer creating a venv and installing the same packages.\n- The model in the repository is `unsloth/meta-llama-3.1-8b-unsloth-bnb-4bit` (~671 MB safetensors). This is small enough to convert to ggml/q4 and run on M1 with quantization.\n- Long-running steps: conversion (minutes), building `llama.cpp` (minutes), and starting the server.\n- Security: if you expose the endpoint with ngrok, protect it with an API key or use the web UI access controls.\n"]},{"cell_type":"code","metadata":{"tags":["colab","install"]},"source":["# Install Python packages (Colab friendly). Long-running network install.\n!pip install -q huggingface_hub safetensors transformers numpy requests pyngrok\n# text-generation-webui has heavy extras; we recommend installing it later when needed (it may take time and more deps).\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{},"source":["## 2) Download the safetensors from Hugging Face\n\nEnter your HF token when prompted (or set `HF_TOKEN` environment variable). We will download into `/content/models/ib-physics` (Colab) or `~/models/ib-physics` locally — adapt paths if needed.\n\n**Note:** If the model repo is public you can omit the token. If private, provide a token with read access.\n"]},{"cell_type":"code","metadata":{"tags":["colab","download"]},"source":["from huggingface_hub import hf_hub_download\nimport os\n\nmodel_id = 'unsloth/meta-llama-3.1-8b-unsloth-bnb-4bit'\n# You can also set HF_TOKEN in the environment before running\nhf_token = os.environ.get('HF_TOKEN') or input('Hugging Face token (or press Enter if public model): ').strip() or None\nout_dir = '/content/models/ib-physics'\nos.makedirs(out_dir, exist_ok=True)\n# Common safetensors filename candidates. If you know exact filename, set it here.\ncandidate_files = ['model.safetensors','pytorch_model.safetensors','pytorch_model.bin','consolidated.00.pth']\nfound = None\nfor fname in candidate_files:\n    try:\n        path = hf_hub_download(repo_id=model_id, filename=fname, repo_type='model', token=hf_token)\n        print('Downloaded:', path)\n        found = path\n        break\n    except Exception as e:\n        # try next candidate\n        pass\n\nif not found:\n    print('\\nCould not auto-detect filename. Visit the model page and copy the safetensors filename, then run hf_hub_download with that filename.')\nelse:\n    print('\\nModel downloaded to:', found)\n    print('You can now use the conversion steps below.')\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{},"source":["## 3) Build llama.cpp (only if you plan to use its converter or the CLI test)\n\nThis builds a native binary on Colab (or your Mac). Mark this as long-running.\n"]},{"cell_type":"code","metadata":{"tags":["colab","build"]},"source":["# clone and build llama.cpp (if not already present)\n!git clone https://github.com/ggerganov/llama.cpp /content/llama.cpp || true\n%cd /content/llama.cpp\n!make -j 2 || true\n%cd /content\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{},"source":["## 4) Convert safetensors → ggml/gguf (preferred: use text-generation-webui converter, fallback: llama.cpp converter)\n\n**WARNING:** This is long-running and memory intensive. Prefer `q4_0` quantization for M1/8GB.\n"]},{"cell_type":"code","metadata":{"tags":["colab","convert"]},"source":["# Example conversion using text-generation-webui converter (if available)\n# The notebook assumes the safetensors path is in `found` variable from the download cell. If not, set SAFE_PATH manually.\ntry:\n    SAFE_PATH\nexcept NameError:\n    SAFE_PATH = None\n\nif SAFE_PATH is None:\n    print('If you have a safetensors file path, set SAFE_PATH = \\'/content/models/ib-physics/model.safetensors\\' and re-run this cell.')\nelse:\n    print('SAFE_PATH:', SAFE_PATH)\n\n# Suggested commands (do not auto-run here unless you understand RAM usage):\nprint('\\nPreferred (text-generation-webui):')\nprint('  git clone https://github.com/oobabooga/text-generation-webui ~/text-generation-webui')\nprint('  cd ~/text-generation-webui')\nprint('  pip install -r requirements.txt')\nprint('  python3 <converter-script> --safetensors', SAFE_PATH, '--out-ggml /content/models/ib-physics/ggml-model-q4_0.bin --quantize q4_0')\n\nprint('\\nFallback (llama.cpp converter):')\nprint('  cd /content/llama.cpp')\nprint('  python3 tools/convert.py --safetensors', SAFE_PATH, '--out /content/models/ib-physics/ggml-model-q4_0.bin --quantize q4_0')\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{},"source":["## 5) Start a local text-generation server (text-generation-webui recommended)\n\nAfter conversion, use the web UI to expose an API. This cell shows the command to run. On Colab you should run it in the background (nohup) and then use ngrok to expose it publicly.\n"]},{"cell_type":"code","metadata":{"tags":["colab","runserver"]},"source":["# Example: start text-generation-webui (adjust path to your model binary)\n# NOTE: This will block the notebook if run normally. Use nohup or run in a separate terminal.\n\nMODEL_BIN = '/content/models/ib-physics/ggml-model-q4_0.bin'\nprint('If you have text-generation-webui cloned at /content/text-generation-webui:')\nprint('  cd /content/text-generation-webui')\nprint('  python server.py --model', MODEL_BIN, '--listen --api')\n\nprint('\\nOn Colab you can run the server in a background process, then use ngrok to expose it (see below).')\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{},"source":["## 6) Expose the server publicly with ngrok (optional)\n\nThis cell installs `pyngrok` and shows how to start an ngrok tunnel to the server port (default 5000 for webui). You must provide an ngrok auth token if you want a stable public URL.\n"]},{"cell_type":"code","metadata":{"tags":["colab","ngrok"]},"source":["# Install pyngrok if not installed\n!pip install -q pyngrok\nfrom pyngrok import ngrok\n\n# If you have an ngrok authtoken, set it here (or run: ngrok authtoken YOUR_TOKEN in shell)\n# ngrok.set_auth_token('YOUR_NGROK_AUTHTOKEN')\n\n# Example: open a tunnel to port 5000\npublic_url = ngrok.connect(5000)\nprint('Public URL:', public_url)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{},"source":["## 7) Test the endpoint (example POST to /api/v1/generate). Adjust path according to server API.\n\nThis cell demonstrates how to call the public/ngrok URL or the local server and print the generated text.\n"]},{"cell_type":"code","metadata":{"tags":["colab","test"]},"source":["import requests\n\n# Replace with your actual URL printed by the ngrok cell or local server:\nTEST_URL = input('Enter public URL from ngrok (or http://localhost:5000): ').strip()\nif TEST_URL.endswith('/'):\n    TEST_URL = TEST_URL[:-1]\n\nAPI_ENDPOINT = TEST_URL + '/api/v1/generate'\nprint('Testing endpoint:', API_ENDPOINT)\n\npayload = {\n    'inputs': 'Generate an IB Physics Paper 1 style multiple-choice question about kinematics',\n    'parameters': { 'max_new_tokens': 200, 'temperature': 0.7 }\n}\n\ntry:\n    res = requests.post(API_ENDPOINT, json=payload, timeout=60)\n    print('status', res.status_code)\n    print(res.text[:1000])\nexcept Exception as e:\n    print('Error calling endpoint:', e)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{},"source":["## 8) Final: environment variables for this repo\n\nAfter you have a public URL (or local server), set the following environment variable in the shell where you start the Next.js app (or in your process manager):\n\n```bash\nexport LOCAL_TGI_URL=\"https://your-public-url.ngrok.io\"\nnpm run dev\n```\n\nThe patched Llama client will prefer that URL and fall back to HF if the URL is not set or unreachable.\n\n---\n\nIf conversion fails or you run into OOM, I recommend using the Hugging Face Inference API (set `HUGGINGFACE_API_KEY` and `LLAMA_MODEL_ID` environment vars) as a robust fallback.\n\nTroubleshooting:\n- If you see OOM during conversion: try a smaller quantization (q4_k) or convert on a machine with more RAM.\n- If web UI fails to start: check the server logs and ensure the model binary path is correct.\n- If ngrok URL returns 502: make sure the server is running and listening on the port you exposed.\n"]}]}